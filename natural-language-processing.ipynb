{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Semantic similarity\n",
    "- Semantic similarity is the task of finding similar sentences that appear in a similar context.\n",
    "- For this task I will use sentence embedding and I will describe you how it can be used in real applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similarity Search in Vector Space\n",
    "- For this example I will use Sentence Embeddings using Siamese BERT-Networks.\n",
    "- To demonstrate the use of vector fields, I will import the pre-trained bert-base-nli-mean-tokens model.\n",
    "- Bidirectional Encoder Representations from Transformers (BERT) is a technique for NLP (Natural Language Processing) pre-training developed by Google.\n",
    "- This model will generate one embedding vector for every sentence. Similar sentences tend to appear in a similar context.\n",
    "- When comparing embedding vectors, it is common to use cosine similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from scipy.spatial.distance import cdist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load pretrained bert model with sentence_transformers library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedder = SentenceTransformer('bert-base-nli-mean-tokens')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Corpus with example sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\n",
    "    \"Compressing / Decompressing Folders & Files\",\n",
    "    \"How do you tell whether a string is an IP or a hostname\",\n",
    "    \"Convert Bytes to Floating Point Numbers in Python\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first generate one embedding for every sentence in a corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 12.7 ms, sys: 22.4 ms, total: 35.1 ms\n",
      "Wall time: 36.4 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "corpus_embeddings = embedder.encode(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One sentence is represented by an embedding vector with 768 numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(768,)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_embeddings[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we generate the embeddings for different query sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 24.3 ms, sys: 0 ns, total: 24.3 ms\n",
      "Wall time: 23 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "queries = [\n",
    "    \"zipping up files\",\n",
    "    \"determine if something is an IP\"\n",
    "]\n",
    "query_embeddings = embedder.encode(queries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then use scipy to find the most-similar embeddings for queries in the corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: zipping up files\n",
      "Top 3 most similar sentences in corpus:\n",
      "\tCompressing / Decompressing Folders & Files (Score 0.7851)\n",
      "\tConvert Bytes to Floating Point Numbers in Python (Score 0.3926)\n",
      "\tHow do you tell whether a string is an IP or a hostname (Score 0.3224)\n",
      "\n",
      "\n",
      "Query: determine if something is an IP\n",
      "Top 3 most similar sentences in corpus:\n",
      "\tHow do you tell whether a string is an IP or a hostname (Score 0.8345)\n",
      "\tConvert Bytes to Floating Point Numbers in Python (Score 0.4138)\n",
      "\tCompressing / Decompressing Folders & Files (Score 0.3728)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for query, query_embedding in zip(queries, query_embeddings):\n",
    "    distances = cdist([query_embedding], corpus_embeddings, \"cosine\")[0]\n",
    "    distances_sorted = np.argsort(distances)\n",
    "    \n",
    "    print('Query: %s' % query)\n",
    "    print('Top 3 most similar sentences in corpus:')\n",
    "    for i in range(3):\n",
    "        print('\\t%s (Score %.4f)' % (sentences[distances_sorted[i]], 1-distances[distances_sorted[i]]))\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similarity Search in Vector Space with Elasticsearch\n",
    "- With Elasticsearch, we can determine textual similarity by using vector embeddings.\n",
    "- For sentence embeddings, the cosine similarity between two sentence vectors can reveal the semantic similarity of the corresponding sentences.\n",
    "- Starting from Elasticsearch 7.2 cosine similarity is available as a predefined function which is usable for document scoring."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We could use text embeddings to allow for retrieving similar sentences:\n",
    "1. During indexing, each question is run through a sentence embedding model to produce a numeric vector.\n",
    "2. When a user enters a query, it is run through the same sentence embedding model to produce a vector.\n",
    "3. To rank the responses, we calculate the vector similarity between each sentence and the query vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limitations\n",
    "- Sentence scoring with cosine similarity is relatively expensive and should be used together with filters to limit the number of sentences for which scores need to be calculated.\n",
    "- For larger scale similarity search in dense vectors, FAISS library for \"billion-scale similarity search with GPUs\" might be a good choice.\n",
    "- This is an example of how embedding models could be used with vector fields, and not as a production-ready solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusions\n",
    "- Embedding techniques provide a powerful way to capture the linguistic content of a piece of text.\n",
    "- By indexing embeddings and scoring based on vector distance, we can compare sentences using a notion of similarity that goes beyond their word-level overlap.\n",
    "- This method can be used together with filters to limit the number of sentences for which scores need to be calculated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Topic Analysis\n",
    "- Topic analysis is a machine learning technique that automatically assigns topics to text data. Topic analysis tools analyze unstructured text, including emails and social media interactions.\n",
    "- There are two different approaches to topic analysis:\n",
    " - Topic modeling: used to discover the main topics within a bunch of texts\n",
    " - Topic classification: used to automatically categorize texts by topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic Modeling\n",
    "- Topic modeling is an unsupervised machine learning technique that’s capable of scanning a set of sentences, detecting word and phrase patterns within them, and automatically clustering word groups and similar expressions that best characterize a set of sentences.\n",
    "- It doesn’t require a predefined list of tags or training data that’s been previously classified by humans.\n",
    "- Topic modeling involves counting words and grouping similar word patterns to infer topics within unstructured data.\n",
    "- By detecting patterns such as word frequency and distance between words, a topic model clusters feedback that is similar, and words and expressions that appear most often. With this information, you can quickly deduce what each set of texts are talking about."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Latent Dirichlet Allocation (LDA)\n",
    "- Latent Dirichlet Allocation (LDA) are based on the underlying assumptions: the distributional hypothesis (similar topics make use of similar words) and the statistical mixture hypothesis (sentences talk about several topics) for which a statistical distribution can be determined.\n",
    "- The purpose of LDA is mapping each sentence to a set of topics which covers a good deal of the words in the sentence.\n",
    "- LDA ignores syntactic information and treats sentences as bags of words. It also assumes that all words in the document can be assigned a probability of belonging to a topic.\n",
    "- The goal of LDA is to determine the mixture of topics that a sentence contains."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LDA assumes that topics and documents look like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/1.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And, when LDA models a new document, it works this way:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/2.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import nltk\n",
    "import re\n",
    "import stop_words\n",
    "\n",
    "tokenizer = nltk.tokenize.RegexpTokenizer(r'\\b[^\\d\\W]+\\b')\n",
    "en_stop = stop_words.get_stop_words('en')\n",
    "lemmatizer = nltk.stem.wordnet.WordNetLemmatizer()\n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    tokens = [word for word in tokens if word not in en_stop]  # remove stop words from tokens\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]  # lemmatize tokens\n",
    "    tokens = [word for word in tokens if len(word) > 1]  # remove word containing less than one char\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load pretrained LDA model with NYTimes News Articles dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = 'nytimes_lda.model'\n",
    "dictionary = gensim.corpora.dictionary.Dictionary.load('dictionary_nytimes')\n",
    "ldamodel = gensim.models.LdaModel.load(MODEL_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model was trained with 25 number of topics. Let's see some of them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topic: 6 - words: ['zika', 'puerto', 'virus', 'rico', 'woman', 'health', 'mosquito', 'tobacco', 'clare', 'pregnancy']\n",
      "topic: 7 - words: ['art', 'work', 'museum', 'artist', 'new', 'mr', 'gallery', 'will', 'fashion', 'said']\n",
      "topic: 0 - words: ['dr', 'said', 'drug', 'study', 'patient', 'health', 'year', 'can', 'medical', 'cancer']\n",
      "topic: 1 - words: ['bank', 'money', 'tax', 'financial', 'pay', 'million', 'cost', 'year', 'fund', 'bond']\n",
      "topic: 23 - words: ['china', 'chinese', 'russian', 'russia', 'north', 'nuclear', 'korea', 'state', 'beijing', 'united']\n",
      "topic: 13 - words: ['game', 'goal', 'first', 'second', 'yankee', 'two', 'season', 'series', 'said', 'scored']\n",
      "topic: 2 - words: ['company', 'percent', 'said', 'year', 'new', 'google', 'like', 'technology', 'facebook', 'service']\n",
      "topic: 21 - words: ['company', 'million', 'billion', 'year', 'business', 'deal', 'sale', 'investor', 'executive', 'said']\n",
      "topic: 9 - words: ['said', 'state', 'law', 'court', 'case', 'will', 'justice', 'federal', 'right', 'year']\n",
      "topic: 14 - words: ['said', 'run', 'two', 'first', 'season', 'hit', 'game', 'last', 'year', 'inning']\n"
     ]
    }
   ],
   "source": [
    "topics = ldamodel.print_topics()\n",
    "for i in range(10):\n",
    "    print('topic: %d - words: %s' % (topics[i][0], re.findall(r\"[a-z|A-Z]+\", topics[i][1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's clean a sentence and transform it into a bag of words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = 'Credit the Cavaliers for playing stout defense and the officials for allowing physical play.'\n",
    "tokenized_data = clean_text(sentence)\n",
    "bow = dictionary.doc2bow(tokenized_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get topic for a sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: Credit the Cavaliers for playing stout defense and the officials for allowing physical play.\n",
      "Topic id: 10 - Similarity Score: 0.732\n",
      "Topic words: ['game', 'point', 'team', 'curry', 'warrior', 'said', 'player', 'season', 'first', 'coach']\n"
     ]
    }
   ],
   "source": [
    "topics = ldamodel.get_document_topics(bow)\n",
    "topics = sorted(topics, key=lambda x: x[1], reverse=True)\n",
    "topic_id, topic_score = topics[0]\n",
    "\n",
    "print('Sentence: %s' % sentence)\n",
    "print('Topic id: %d - Similarity Score: %.3f' % (topic_id, topic_score))\n",
    "print('Topic words: %s' % re.findall(r\"[a-z|A-Z]+\", ldamodel.print_topic(topic_id)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some notes about LDA Model from gensim:\n",
    " * Is streamed: training documents may come in sequentially, no random access required.\n",
    " * Runs in constant memory w.r.t. the number of documents: size of the training corpus does not affect memory footprint, can process corpora larger than RAM.\n",
    " * Is distributed: makes use of a cluster of machines, if available, to speed up model estimation.\n",
    " * This module allows both LDA model estimation from a training corpus and inference of topic distribution on new, unseen documents. The model can also be updated with new documents for online training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic Classification\n",
    "- Topic classification is a supervised machine learning technique, one that needs training before being able to automatically analyze texts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the text is transformed into vectors and the training data is tagged with the expected tags, this information is fed to an algorithm to create the classification model:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/5.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This classification model is now able to classify new texts because it has learned how to make predictions automatically:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/6.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Cases & Applications\n",
    "- Topic modeling and topic classification can automatically tag user message according to a specific topic.\n",
    "- This can be used in a query to retrieve similar messages from a database."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Other methods to extract relevant information from texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Named Entity Recognition:\n",
    "* Named entity extractors locate and classify named entities, like names, organizations, locations, and monetary values, in unstructured texts. AI programs recognize these titles and values through their unique word sequences, and then classify them.\n",
    "* Named entity extraction can be used to reveal important data and provide content recommendations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/4.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keyword Extraction\n",
    "* Keyword extraction extracts relevant terms and phrases from within a text. These are terms that help to summarize the text, are significant to the writer’s viewpoint, or significant to the overall concept of the text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/3.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intent Classification\n",
    "* Intent classification automatically finds purpose and goals in text.\n",
    "* With an intent classifier, you could easily locate this query among the numerous user interactions you receive on a daily basis, and automatically categorize it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final thoughts\n",
    "* These methods enumerated above can be combined in order to make better suggestions to the user.\n",
    "* Topic Analysis, Named Entity Recognition, Keyword Extraction and Intent Classification can be used to extract relevant information from text.\n",
    "* Semantic Similarity should be used to find sentences that appear in a similar context with a given query and should be used together with filters (Topic Analysis, NER, Keywords and Intent Labels) to limit the number of sentences for which scores need to be calculated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "1. Papers:\n",
    " * Attention Is All You Need: https://arxiv.org/pdf/1706.03762.pdf\n",
    " * BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding: https://arxiv.org/pdf/1810.04805.pdf\n",
    " * Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks: https://arxiv.org/pdf/1908.10084.pdf\n",
    " * Latent Dirichlet Allocation: http://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf\n",
    "2. Libraries:\n",
    " * https://github.com/UKPLab/sentence-transformers\n",
    " * https://github.com/facebookresearch/faiss\n",
    " * https://github.com/deepmipt/DeepPavlov/tree/master/examples\n",
    "3. Interesting topics:\n",
    " * Google’s Universal Sentence Encoder: https://ai.googleblog.com/2018/05/advances-in-semantic-textual-similarity.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Contact me:\n",
    "- www.linkedin.com/in/catalinmelter/\n",
    "- catalin.melter@gmail.com"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
